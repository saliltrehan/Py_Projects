{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pymongo\n",
    "from common import *\n",
    "import pandas as pd\n",
    "\n",
    "#Removes non-alphabetic characters:\n",
    "def text_strip(column):\n",
    "    for row in column:\n",
    "        \n",
    "        #ORDER OF REGEX IS VERY VERY IMPORTANT!!!!!!\n",
    "        \n",
    "        row=re.sub(\"(\\\\t)\", ' ', str(row)).lower() #remove escape charecters\n",
    "        row=re.sub(\"(\\\\r)\", ' ', str(row)).lower() \n",
    "        row=re.sub(\"(\\\\n)\", ' ', str(row)).lower()\n",
    "        \n",
    "        row=re.sub(\"(__+)\", ' ', str(row)).lower()   #remove _ if it occors more than one time consecutively\n",
    "        row=re.sub(\"(--+)\", ' ', str(row)).lower()   #remove - if it occors more than one time consecutively\n",
    "        row=re.sub(\"(~~+)\", ' ', str(row)).lower()   #remove ~ if it occors more than one time consecutively\n",
    "        row=re.sub(\"(\\+\\++)\", ' ', str(row)).lower()   #remove + if it occors more than one time consecutively\n",
    "        row=re.sub(\"(\\.\\.+)\", ' ', str(row)).lower()   #remove . if it occors more than one time consecutively\n",
    "        \n",
    "        row=re.sub(r\"[<>()|&©ø\\[\\]\\'\\\",;?~*!]\", ' ', str(row)).lower() #remove <>()|&©ø\"',;?~*!\n",
    "        \n",
    "        row=re.sub(\"(mailto:)\", ' ', str(row)).lower() #remove mailto:\n",
    "        row=re.sub(r\"(\\\\x9\\d)\", ' ', str(row)).lower() #remove \\x9* in text\n",
    "        row=re.sub(\"([iI][nN][cC]\\d+)\", 'INC_NUM', str(row)).lower() #replace INC nums to INC_NUM\n",
    "        row=re.sub(\"([cC][mM]\\d+)|([cC][hH][gG]\\d+)\", 'CM_NUM', str(row)).lower() #replace CM# and CHG# to CM_NUM\n",
    "        \n",
    "        \n",
    "        row=re.sub(\"(\\.\\s+)\", ' ', str(row)).lower() #remove full stop at end of words(not between)\n",
    "        row=re.sub(\"(\\-\\s+)\", ' ', str(row)).lower() #remove - at end of words(not between)\n",
    "        row=re.sub(\"(\\:\\s+)\", ' ', str(row)).lower() #remove : at end of words(not between)\n",
    "        \n",
    "        row=re.sub(\"(\\s+.\\s+)\", ' ', str(row)).lower() #remove any single charecters hanging between 2 spaces\n",
    "        \n",
    "        #Replace any url as such https://abc.xyz.net/browse/sdf-5327 ====> abc.xyz.net\n",
    "        try:\n",
    "            url = re.search(r'((https*:\\/*)([^\\/\\s]+))(.[^\\s]+)', str(row))\n",
    "            repl_url = url.group(3)\n",
    "            row = re.sub(r'((https*:\\/*)([^\\/\\s]+))(.[^\\s]+)',repl_url, str(row))\n",
    "        except:\n",
    "            pass #there might be emails with no url in them\n",
    "        \n",
    "\n",
    "        \n",
    "        row = re.sub(\"(\\s+)\",' ',str(row)).lower() #remove multiple spaces\n",
    "        \n",
    "        #Should always be last\n",
    "        row=re.sub(\"(\\s+.\\s+)\", ' ', str(row)).lower() #remove any single charecters hanging between 2 spaces\n",
    "\n",
    "        \n",
    "        \n",
    "        yield row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(news_data.find({}, {\"_id\": 0, \"headlines\": 1,\t\"text\":1}))\n",
    "\n",
    "\n",
    "df.rename(columns={\"headlines\": \"summary\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40324, 2)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "brief_cleaning1 = text_strip(df['text'])\n",
    "brief_cleaning2 = text_strip(df['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /home/salils/Documents/Python_n_R/Py_Projects/env/lib/python3.10/site-packages (3.4.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/salils/Documents/Python_n_R/Py_Projects/env/lib/python3.10/site-packages (from spacy) (1.0.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /home/salils/Documents/Python_n_R/Py_Projects/env/lib/python3.10/site-packages (from spacy) (1.9.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/salils/Documents/Python_n_R/Py_Projects/env/lib/python3.10/site-packages (from spacy) (1.0.7)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/salils/Documents/Python_n_R/Py_Projects/env/lib/python3.10/site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/salils/Documents/Python_n_R/Py_Projects/env/lib/python3.10/site-packages (from spacy) (2.28.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/salils/Documents/Python_n_R/Py_Projects/env/lib/python3.10/site-packages (from spacy) (2.4.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/salils/Documents/Python_n_R/Py_Projects/env/lib/python3.10/site-packages (from spacy) (4.64.0)\n",
      "Requirement already satisfied: jinja2 in /home/salils/Documents/Python_n_R/Py_Projects/env/lib/python3.10/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/salils/Documents/Python_n_R/Py_Projects/env/lib/python3.10/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/salils/Documents/Python_n_R/Py_Projects/env/lib/python3.10/site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /home/salils/Documents/Python_n_R/Py_Projects/env/lib/python3.10/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/salils/Documents/Python_n_R/Py_Projects/env/lib/python3.10/site-packages (from spacy) (0.6.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/salils/Documents/Python_n_R/Py_Projects/env/lib/python3.10/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /home/salils/Documents/Python_n_R/Py_Projects/env/lib/python3.10/site-packages (from spacy) (0.4.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/salils/Documents/Python_n_R/Py_Projects/env/lib/python3.10/site-packages (from spacy) (1.23.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /home/salils/Documents/Python_n_R/Py_Projects/env/lib/python3.10/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: setuptools in /home/salils/Documents/Python_n_R/Py_Projects/env/lib/python3.10/site-packages (from spacy) (59.6.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/salils/Documents/Python_n_R/Py_Projects/env/lib/python3.10/site-packages (from spacy) (3.0.6)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /home/salils/Documents/Python_n_R/Py_Projects/env/lib/python3.10/site-packages (from spacy) (8.1.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/salils/Documents/Python_n_R/Py_Projects/env/lib/python3.10/site-packages (from packaging>=20.0->spacy) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /home/salils/Documents/Python_n_R/Py_Projects/env/lib/python3.10/site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/salils/Documents/Python_n_R/Py_Projects/env/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy) (4.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/salils/Documents/Python_n_R/Py_Projects/env/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/salils/Documents/Python_n_R/Py_Projects/env/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/salils/Documents/Python_n_R/Py_Projects/env/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/salils/Documents/Python_n_R/Py_Projects/env/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.1.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/salils/Documents/Python_n_R/Py_Projects/env/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.8)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/salils/Documents/Python_n_R/Py_Projects/env/lib/python3.10/site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/salils/Documents/Python_n_R/Py_Projects/env/lib/python3.10/site-packages (from jinja2->spacy) (2.1.1)\n",
      "2022-07-30 10:48:49.129847: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-30 10:48:49.129872: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-07-30 10:48:50.819946: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-07-30 10:48:50.819975: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (salils-Legion-5-15ACH6): /proc/driver/nvidia/version does not exist\n",
      "Collecting en-core-web-sm==3.4.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.0/en_core_web_sm-3.4.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /home/salils/Documents/Python_n_R/Py_Projects/env/lib/python3.10/site-packages (from en-core-web-sm==3.4.0) (3.4.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/salils/Documents/Python_n_R/Py_Projects/env/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.3)\n",
      "Requirement already satisfied: setuptools in /home/salils/Documents/Python_n_R/Py_Projects/env/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (59.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/salils/Documents/Python_n_R/Py_Projects/env/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (21.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/salils/Documents/Python_n_R/Py_Projects/env/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.28.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /home/salils/Documents/Python_n_R/Py_Projects/env/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.9)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /home/salils/Documents/Python_n_R/Py_Projects/env/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.9.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/salils/Documents/Python_n_R/Py_Projects/env/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.64.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/salils/Documents/Python_n_R/Py_Projects/env/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.23.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/salils/Documents/Python_n_R/Py_Projects/env/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.6.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/salils/Documents/Python_n_R/Py_Projects/env/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.8)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/salils/Documents/Python_n_R/Py_Projects/env/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.6)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/salils/Documents/Python_n_R/Py_Projects/env/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.7)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /home/salils/Documents/Python_n_R/Py_Projects/env/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (8.1.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/salils/Documents/Python_n_R/Py_Projects/env/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.4.4)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /home/salils/Documents/Python_n_R/Py_Projects/env/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.10.1)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /home/salils/Documents/Python_n_R/Py_Projects/env/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/salils/Documents/Python_n_R/Py_Projects/env/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.1.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/salils/Documents/Python_n_R/Py_Projects/env/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.3.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/salils/Documents/Python_n_R/Py_Projects/env/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.6)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/salils/Documents/Python_n_R/Py_Projects/env/lib/python3.10/site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /home/salils/Documents/Python_n_R/Py_Projects/env/lib/python3.10/site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/salils/Documents/Python_n_R/Py_Projects/env/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/salils/Documents/Python_n_R/Py_Projects/env/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/salils/Documents/Python_n_R/Py_Projects/env/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/salils/Documents/Python_n_R/Py_Projects/env/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/salils/Documents/Python_n_R/Py_Projects/env/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.26.11)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/salils/Documents/Python_n_R/Py_Projects/env/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.7.8)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/salils/Documents/Python_n_R/Py_Projects/env/lib/python3.10/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/salils/Documents/Python_n_R/Py_Projects/env/lib/python3.10/site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# !pip3 install spacy\n",
    "# !python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to clean up everything: 1.55 mins\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import spacy\n",
    "\n",
    "# disabling Named Entity Recognition for speed\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser']) \n",
    "\n",
    "#Taking advantage of spaCy .pipe() method to speed-up the cleaning process:\n",
    "#If data loss seems to be happening(i.e len(text) = 50 instead of 75 etc etc) in this cell , decrease the batch_size parametre \n",
    "\n",
    "t = time()\n",
    "\n",
    "# Batch the data points into 5000 and run on all cores for faster preprocessing\n",
    "text = [str(doc) for doc in nlp.pipe(brief_cleaning1, batch_size=5000)]\n",
    "\n",
    "# Takes 7-8 mins\n",
    "print('Time to clean up everything: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to clean up everything: 0.31 mins\n"
     ]
    }
   ],
   "source": [
    "#Taking advantage of spaCy .pipe() method to speed-up the cleaning process:\n",
    "\n",
    "\n",
    "t = time()\n",
    "\n",
    "#Batch the data points into 5000 and run on all cores for faster preprocessing\n",
    "summary = ['_START_ '+ str(doc) + ' _END_' for doc in nlp.pipe(brief_cleaning2, batch_size=5000)]\n",
    "\n",
    "#Takes 7-8 mins\n",
    "print('Time to clean up everything: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_text'] = pd.Series(text)\n",
    "df['cleaned_summary'] = pd.Series(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_count = []\n",
    "summary_count = []\n",
    "\n",
    "for sent in df['cleaned_text']:\n",
    "    text_count.append(len(sent.split()))\n",
    "for sent in df['cleaned_summary']:\n",
    "    summary_count.append(len(sent.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "graph_df= pd.DataFrame()\n",
    "graph_df['text']=text_count\n",
    "graph_df['summary']=summary_count\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "graph_df.hist(bins = 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8992411467116358\n"
     ]
    }
   ],
   "source": [
    "#Check how much % of summary have 0-15 words\n",
    "cnt=0\n",
    "for i in df['cleaned_summary']:\n",
    "    if(len(i.split())<=15):\n",
    "        cnt=cnt+1\n",
    "print(cnt/len(df['cleaned_summary']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "#Check how much % of text have 0-70 words\n",
    "cnt=0\n",
    "for i in df['cleaned_text']:\n",
    "    if(len(i.split())<=100):\n",
    "        cnt=cnt+1\n",
    "print(cnt/len(df['cleaned_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install numpy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>indian equity benchmarks rallied on thursday l...</td>\n",
       "      <td>_START_ sensex jumps 000 points nifty crosses ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>shares of spicejet tanked as much as 10% on th...</td>\n",
       "      <td>_START_ spicejet shares hit 2-year low after a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  indian equity benchmarks rallied on thursday l...   \n",
       "1  shares of spicejet tanked as much as 10% on th...   \n",
       "\n",
       "                                             summary  \n",
       "0  _START_ sensex jumps 000 points nifty crosses ...  \n",
       "1  _START_ spicejet shares hit 2-year low after a...  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model to summarize the text between 0-15 words for Summary and 0-100 words for Text\n",
    "max_text_len=100\n",
    "max_summary_len=15\n",
    "\n",
    "#Select the Summaries and Text between max len defined above\n",
    "\n",
    "cleaned_text =np.array(df['cleaned_text'])\n",
    "cleaned_summary=np.array(df['cleaned_summary'])\n",
    "\n",
    "short_text=[]\n",
    "short_summary=[]\n",
    "\n",
    "for i in range(len(cleaned_text)):\n",
    "    if(len(cleaned_summary[i].split())<=max_summary_len and len(cleaned_text[i].split())<=max_text_len):\n",
    "        short_text.append(cleaned_text[i])\n",
    "        short_summary.append(cleaned_summary[i])\n",
    "        \n",
    "post_pre=pd.DataFrame({'text':short_text,'summary':short_summary})\n",
    "\n",
    "post_pre.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add sostok and eostok at \n",
    "post_pre['summary'] = post_pre['summary'].apply(lambda x : 'sostok '+ x + ' eostok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>indian equity benchmarks rallied on thursday l...</td>\n",
       "      <td>sostok _START_ sensex jumps 000 points nifty c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>shares of spicejet tanked as much as 10% on th...</td>\n",
       "      <td>sostok _START_ spicejet shares hit 2-year low ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>amid report of lower inventories in the us and...</td>\n",
       "      <td>sostok _START_ oil price rises as russia cuts ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>meta has recorded its first ever quarterly dro...</td>\n",
       "      <td>sostok _START_ meta posts its first ever quart...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hcl technologies chairperson roshni nadar malh...</td>\n",
       "      <td>sostok _START_ who are the richest indian wome...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  indian equity benchmarks rallied on thursday l...   \n",
       "1  shares of spicejet tanked as much as 10% on th...   \n",
       "2  amid report of lower inventories in the us and...   \n",
       "3  meta has recorded its first ever quarterly dro...   \n",
       "4  hcl technologies chairperson roshni nadar malh...   \n",
       "\n",
       "                                             summary  \n",
       "0  sostok _START_ sensex jumps 000 points nifty c...  \n",
       "1  sostok _START_ spicejet shares hit 2-year low ...  \n",
       "2  sostok _START_ oil price rises as russia cuts ...  \n",
       "3  sostok _START_ meta posts its first ever quart...  \n",
       "4  sostok _START_ who are the richest indian wome...  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_pre.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_tr,x_val,y_tr,y_val=train_test_split(np.array(post_pre['text']),np.array(post_pre['summary']),test_size=0.1,random_state=0,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets tokenize the text to get the vocab count , you can use Spacy here also\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "#prepare a tokenizer for reviews on training data\n",
    "x_tokenizer = Tokenizer() \n",
    "x_tokenizer.fit_on_texts(list(x_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of rare words in vocabulary: 58.19164619164619\n",
      "Total Coverage of rare words: 2.0576364011692423\n"
     ]
    }
   ],
   "source": [
    "thresh=4\n",
    "\n",
    "cnt=0\n",
    "tot_cnt=0\n",
    "freq=0\n",
    "tot_freq=0\n",
    "\n",
    "for key,value in x_tokenizer.word_counts.items():\n",
    "    tot_cnt=tot_cnt+1\n",
    "    tot_freq=tot_freq+value\n",
    "    if(value<thresh):\n",
    "        cnt=cnt+1\n",
    "        freq=freq+value\n",
    "    \n",
    "print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n",
    "print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary in X = 377\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#prepare a tokenizer for reviews on training data\n",
    "x_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \n",
    "x_tokenizer.fit_on_texts(list(x_tr))\n",
    "\n",
    "#convert text sequences into integer sequences (i.e one-hot encodeing all the words)\n",
    "x_tr_seq    =   x_tokenizer.texts_to_sequences(x_tr) \n",
    "x_val_seq   =   x_tokenizer.texts_to_sequences(x_val)\n",
    "\n",
    "#padding zero upto maximum length\n",
    "x_tr    =   pad_sequences(x_tr_seq,  maxlen=max_text_len, padding='post')\n",
    "x_val   =   pad_sequences(x_val_seq, maxlen=max_text_len, padding='post')\n",
    "\n",
    "#size of vocabulary ( +1 for padding token)\n",
    "x_voc   =  x_tokenizer.num_words + 1\n",
    "\n",
    "print(\"Size of vocabulary in X = {}\".format(x_voc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare a tokenizer for reviews on training data\n",
    "y_tokenizer = Tokenizer()   \n",
    "y_tokenizer.fit_on_texts(list(y_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of rare words in vocabulary: 72.40384615384615\n",
      "Total Coverage of rare words: 6.159802742852485\n"
     ]
    }
   ],
   "source": [
    "thresh=6\n",
    "\n",
    "cnt=0\n",
    "tot_cnt=0\n",
    "freq=0\n",
    "tot_freq=0\n",
    "\n",
    "for key,value in y_tokenizer.word_counts.items():\n",
    "    tot_cnt=tot_cnt+1\n",
    "    tot_freq=tot_freq+value\n",
    "    if(value<thresh):\n",
    "        cnt=cnt+1\n",
    "        freq=freq+value\n",
    "    \n",
    "print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n",
    "print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary in Y = 6315\n"
     ]
    }
   ],
   "source": [
    "#prepare a tokenizer for reviews on training data\n",
    "y_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \n",
    "y_tokenizer.fit_on_texts(list(y_tr))\n",
    "\n",
    "#convert text sequences into integer sequences (i.e one hot encode the text in Y)\n",
    "y_tr_seq    =   y_tokenizer.texts_to_sequences(y_tr) \n",
    "y_val_seq   =   y_tokenizer.texts_to_sequences(y_val) \n",
    "\n",
    "#padding zero upto maximum length\n",
    "y_tr    =   pad_sequences(y_tr_seq, maxlen=max_summary_len, padding='post')\n",
    "y_val   =   pad_sequences(y_val_seq, maxlen=max_summary_len, padding='post')\n",
    "\n",
    "#size of vocabulary\n",
    "y_voc  =   y_tokenizer.num_words +1\n",
    "print(\"Size of vocabulary in Y = {}\".format(y_voc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind=[]\n",
    "for i in range(len(y_tr)):\n",
    "    cnt=0\n",
    "    for j in y_tr[i]:\n",
    "        if j!=0:\n",
    "            cnt=cnt+1\n",
    "    if(cnt==2):\n",
    "        ind.append(i)\n",
    "\n",
    "y_tr=np.delete(y_tr,ind, axis=0)\n",
    "x_tr=np.delete(x_tr,ind, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind=[]\n",
    "for i in range(len(y_val)):\n",
    "    cnt=0\n",
    "    for j in y_val[i]:\n",
    "        if j!=0:\n",
    "            cnt=cnt+1\n",
    "    if(cnt==2):\n",
    "        ind.append(i)\n",
    "\n",
    "y_val=np.delete(y_val,ind, axis=0)\n",
    "x_val=np.delete(x_val,ind, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary from the w2v model = 377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-30 10:57:38.023064: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-07-30 10:57:38.023096: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (salils-Legion-5-15ACH6): /proc/driver/nvidia/version does not exist\n",
      "2022-07-30 10:57:38.023712: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 100, 200)     75400       ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    [(None, 100, 300),   601200      ['embedding[0][0]']              \n",
      "                                 (None, 300),                                                     \n",
      "                                 (None, 300)]                                                     \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  [(None, 100, 300),   721200      ['lstm[0][0]']                   \n",
      "                                 (None, 300),                                                     \n",
      "                                 (None, 300)]                                                     \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, None, 200)    1263000     ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)                  [(None, 100, 300),   721200      ['lstm_1[0][0]']                 \n",
      "                                 (None, 300),                                                     \n",
      "                                 (None, 300)]                                                     \n",
      "                                                                                                  \n",
      " lstm_3 (LSTM)                  [(None, None, 300),  601200      ['embedding_1[0][0]',            \n",
      "                                 (None, 300),                     'lstm_2[0][1]',                 \n",
      "                                 (None, 300)]                     'lstm_2[0][2]']                 \n",
      "                                                                                                  \n",
      " time_distributed (TimeDistribu  (None, None, 6315)  1900815     ['lstm_3[0][0]']                 \n",
      " ted)                                                                                             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,884,015\n",
      "Trainable params: 5,884,015\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K \n",
    "import gensim\n",
    "from numpy import *\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import warnings\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"Size of vocabulary from the w2v model = {}\".format(x_voc))\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "latent_dim = 300\n",
    "embedding_dim=200\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(max_text_len,))\n",
    "\n",
    "#embedding layer\n",
    "enc_emb =  Embedding(x_voc, embedding_dim,trainable=True)(encoder_inputs)\n",
    "\n",
    "#encoder lstm 1\n",
    "encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "\n",
    "#encoder lstm 2\n",
    "encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "#encoder lstm 3\n",
    "encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True,dropout=0.4,recurrent_dropout=0.4)\n",
    "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "#embedding layer\n",
    "dec_emb_layer = Embedding(y_voc, embedding_dim,trainable=True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True,dropout=0.4,recurrent_dropout=0.2)\n",
    "decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c])\n",
    "\n",
    "#dense layer\n",
    "decoder_dense =  TimeDistributed(Dense(y_voc, activation='softmax'))\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model \n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "255/255 [==============================] - 554s 2s/step - loss: 5.4998 - val_loss: 5.0746\n",
      "Epoch 2/50\n",
      "255/255 [==============================] - 547s 2s/step - loss: 5.0121 - val_loss: 4.8347\n",
      "Epoch 3/50\n",
      "255/255 [==============================] - 555s 2s/step - loss: 4.7900 - val_loss: 4.6657\n",
      "Epoch 4/50\n",
      "255/255 [==============================] - 546s 2s/step - loss: 4.6225 - val_loss: 4.5464\n",
      "Epoch 5/50\n",
      "255/255 [==============================] - 533s 2s/step - loss: 4.4830 - val_loss: 4.4378\n",
      "Epoch 6/50\n",
      "110/255 [===========>..................] - ETA: 5:02 - loss: 4.3494"
     ]
    }
   ],
   "source": [
    "history=model.fit([x_tr,y_tr[:,:-1]], y_tr.reshape(y_tr.shape[0],y_tr.shape[1], 1)[:,1:] ,epochs=50,callbacks=[es],batch_size=128, validation_data=([x_val,y_val[:,:-1]], y_val.reshape(y_val.shape[0],y_val.shape[1], 1)[:,1:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_target_word_index=y_tokenizer.index_word\n",
    "reverse_source_word_index=x_tokenizer.index_word\n",
    "target_word_index=y_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the input sequence to get the feature vector\n",
    "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "# Decoder setup\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_hidden_state_input = Input(shape=(max_text_len,latent_dim))\n",
    "\n",
    "# Get the embeddings of the decoder sequence\n",
    "dec_emb2= dec_emb_layer(decoder_inputs) \n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2) \n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs2] + [state_h2, state_c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "    \n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    \n",
    "    # Populate the first word of target sequence with the start word.\n",
    "    target_seq[0, 0] = target_word_index['sostok']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "      \n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
    "        \n",
    "        if(sampled_token!='eostok'):\n",
    "            decoded_sentence += ' '+sampled_token\n",
    "\n",
    "        # Exit condition: either hit max length or find stop word.\n",
    "        if (sampled_token == 'eostok'  or len(decoded_sentence.split()) >= (max_summary_len-1)):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update internal states\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2summary(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if((i!=0 and i!=target_word_index['sostok']) and i!=target_word_index['eostok']):\n",
    "            newString=newString+reverse_target_word_index[i]+' '\n",
    "    return newString\n",
    "\n",
    "def seq2text(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if(i!=0):\n",
    "            newString=newString+reverse_source_word_index[i]+' '\n",
    "    return newString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,100):\n",
    "    print(\"Review:\",seq2text(x_tr[i]))\n",
    "    print(\"Original summary:\",seq2summary(y_tr[i]))\n",
    "    print(\"Predicted summary:\",decode_sequence(x_tr[i].reshape(1,max_text_len)))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "206a4eb7b5a17dfc707ac57a752c23d13e7684ef023e476fa29bcc3fe1d0fce0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
